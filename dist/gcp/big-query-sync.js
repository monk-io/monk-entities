
// Generated by MonkEC - targeting Goja runtime
var __create = Object.create;
var __defProp = Object.defineProperty;
var __getOwnPropDesc = Object.getOwnPropertyDescriptor;
var __knownSymbol = (name, symbol) => (symbol = Symbol[name]) ? symbol : Symbol.for("Symbol." + name);
var __typeError = (msg) => {
  throw TypeError(msg);
};
var __defNormalProp = (obj, key, value) => key in obj ? __defProp(obj, key, { enumerable: true, configurable: true, writable: true, value }) : obj[key] = value;
var __name = (target, value) => __defProp(target, "name", { value, configurable: true });
var __decoratorStart = (base) => [, , , __create(base?.[__knownSymbol("metadata")] ?? null)];
var __decoratorStrings = ["class", "method", "getter", "setter", "accessor", "field", "value", "get", "set"];
var __expectFn = (fn) => fn !== void 0 && typeof fn !== "function" ? __typeError("Function expected") : fn;
var __decoratorContext = (kind, name, done, metadata, fns) => ({ kind: __decoratorStrings[kind], name, metadata, addInitializer: (fn) => done._ ? __typeError("Already initialized") : fns.push(__expectFn(fn || null)) });
var __decoratorMetadata = (array, target) => __defNormalProp(target, __knownSymbol("metadata"), array[3]);
var __runInitializers = (array, flags, self, value) => {
  for (var i = 0, fns = array[flags >> 1], n = fns && fns.length; i < n; i++) flags & 1 ? fns[i].call(self) : value = fns[i].call(self, value);
  return value;
};
var __decorateElement = (array, flags, name, decorators, target, extra) => {
  var fn, it, done, ctx, access, k = flags & 7, s = !!(flags & 8), p = !!(flags & 16);
  var j = k > 3 ? array.length + 1 : k ? s ? 1 : 2 : 0, key = __decoratorStrings[k + 5];
  var initializers = k > 3 && (array[j - 1] = []), extraInitializers = array[j] || (array[j] = []);
  var desc = k && (!p && !s && (target = target.prototype), k < 5 && (k > 3 || !p) && __getOwnPropDesc(k < 4 ? target : { get [name]() {
    return __privateGet(this, extra);
  }, set [name](x) {
    return __privateSet(this, extra, x);
  } }, name));
  k ? p && k < 4 && __name(extra, (k > 2 ? "set " : k > 1 ? "get " : "") + name) : __name(target, name);
  for (var i = decorators.length - 1; i >= 0; i--) {
    ctx = __decoratorContext(k, name, done = {}, array[3], extraInitializers);
    if (k) {
      ctx.static = s, ctx.private = p, access = ctx.access = { has: p ? (x) => __privateIn(target, x) : (x) => name in x };
      if (k ^ 3) access.get = p ? (x) => (k ^ 1 ? __privateGet : __privateMethod)(x, target, k ^ 4 ? extra : desc.get) : (x) => x[name];
      if (k > 2) access.set = p ? (x, y) => __privateSet(x, target, y, k ^ 4 ? extra : desc.set) : (x, y) => x[name] = y;
    }
    it = (0, decorators[i])(k ? k < 4 ? p ? extra : desc[key] : k > 4 ? void 0 : { get: desc.get, set: desc.set } : target, ctx), done._ = 1;
    if (k ^ 4 || it === void 0) __expectFn(it) && (k > 4 ? initializers.unshift(it) : k ? p ? extra = it : desc[key] = it : target = it);
    else if (typeof it !== "object" || it === null) __typeError("Object expected");
    else __expectFn(fn = it.get) && (desc.get = fn), __expectFn(fn = it.set) && (desc.set = fn), __expectFn(fn = it.init) && initializers.unshift(fn);
  }
  return k || __decoratorMetadata(array, target), desc && __defProp(target, name, desc), p ? k ^ 4 ? extra : desc : target;
};
var __publicField = (obj, key, value) => __defNormalProp(obj, typeof key !== "symbol" ? key + "" : key, value);
var __accessCheck = (obj, member, msg) => member.has(obj) || __typeError("Cannot " + msg);
var __privateIn = (member, obj) => Object(obj) !== obj ? __typeError('Cannot use the "in" operator on this value') : member.has(obj);
var __privateGet = (obj, member, getter) => (__accessCheck(obj, member, "read from private field"), getter ? getter.call(obj) : member.get(obj));
var __privateSet = (obj, member, value, setter) => (__accessCheck(obj, member, "write to private field"), setter ? setter.call(obj, value) : member.set(obj, value), value);
var __privateMethod = (obj, member, method) => (__accessCheck(obj, member, "access private method"), method);

// input/gcp/bigQuery.ts
const base = require("monkec/base");
const action = base.action;
const gcpBase = require("gcp/gcp-base");
const GcpEntity = gcpBase.GcpEntity;
const cli = require("cli");
const common = require("gcp/common");
const BIGQUERY_API_URL = common.BIGQUERY_API_URL;
var _timeTravelInfo_dec, _restoreSnapshot_dec, _deleteSnapshot_dec, _describeSnapshot_dec, _listSnapshots_dec, _createSnapshot_dec, _getBackupInfo_dec, _deleteTable_dec, _createTable_dec, _listAllTables_dec, _getInfo_dec, _a, _init;
var _BigQuery = class _BigQuery extends (_a = GcpEntity, _getInfo_dec = [action("get")], _listAllTables_dec = [action("list-tables")], _createTable_dec = [action("create-table")], _deleteTable_dec = [action("delete-table")], _getBackupInfo_dec = [action("get-backup-info")], _createSnapshot_dec = [action("create-snapshot")], _listSnapshots_dec = [action("list-snapshots")], _describeSnapshot_dec = [action("describe-snapshot")], _deleteSnapshot_dec = [action("delete-snapshot")], _restoreSnapshot_dec = [action("restore")], _timeTravelInfo_dec = [action("time-travel-info")], _a) {
  constructor() {
    super(...arguments);
    __runInitializers(_init, 5, this);
  }
  getEntityName() {
    return `BigQuery Dataset ${this.definition.dataset}`;
  }
  /**
   * Get the API base URL for this project
   */
  get apiUrl() {
    return `${BIGQUERY_API_URL}/projects/${this.projectId}`;
  }
  /**
   * Get dataset details from API
   */
  getDataset() {
    return this.checkResourceExists(
      `${this.apiUrl}/datasets/${this.definition.dataset}`
    );
  }
  /**
   * List tables in the dataset
   */
  listTables() {
    return this.get(
      `${this.apiUrl}/datasets/${this.definition.dataset}/tables`
    );
  }
  /**
   * Delete all tables in the dataset
   */
  deleteAllTables() {
    const tableList = this.listTables();
    if (tableList.tables && tableList.tables.length > 0) {
      cli.output(`Deleting ${tableList.tables.length} tables from dataset`);
      for (const table of tableList.tables) {
        const tableId = table.tableReference.tableId;
        cli.output(`Deleting table: ${tableId}`);
        this.httpDelete(
          `${this.apiUrl}/datasets/${this.definition.dataset}/tables/${tableId}`
        );
      }
    }
  }
  create() {
    const existing = this.getDataset();
    if (existing) {
      cli.output(
        `Dataset ${this.definition.dataset} already exists, adopting...`
      );
      this.state.existing = true;
      this.state.dataset_id = existing.datasetReference.datasetId;
      this.state.dataset_reference = `${this.projectId}:${existing.datasetReference.datasetId}`;
      this.state.self_link = existing.selfLink;
      this.state.location = existing.location;
      this.state.creation_time = existing.creationTime;
      this.state.last_modified_time = existing.lastModifiedTime;
      this.state.storage_billing_model = existing.storageBillingModel;
      return;
    }
    const body = {
      datasetReference: {
        datasetId: this.definition.dataset,
        projectId: this.projectId
      }
    };
    if (this.definition.dataset_description) {
      body.description = this.definition.dataset_description;
    }
    if (this.definition.location) {
      body.location = this.definition.location;
    }
    if (this.definition.default_table_expiration_ms) {
      body.defaultTableExpirationMs = this.definition.default_table_expiration_ms.toString();
    }
    if (this.definition.default_partition_expiration_ms) {
      body.defaultPartitionExpirationMs = this.definition.default_partition_expiration_ms.toString();
    }
    if (this.definition.labels) {
      body.labels = this.definition.labels;
    }
    if (this.definition.storage_billing_model) {
      body.storageBillingModel = this.definition.storage_billing_model;
    }
    if (this.definition.max_time_travel_hours) {
      body.maxTimeTravelHours = this.definition.max_time_travel_hours.toString();
    }
    if (this.definition.is_case_insensitive !== void 0) {
      body.isCaseInsensitive = this.definition.is_case_insensitive;
    }
    if (this.definition.default_collation) {
      body.defaultCollation = this.definition.default_collation;
    }
    cli.output(`Creating BigQuery dataset: ${this.definition.dataset}`);
    const result = this.post(`${this.apiUrl}/datasets`, body);
    this.state.dataset_id = result.datasetReference.datasetId;
    this.state.dataset_reference = `${this.projectId}:${result.datasetReference.datasetId}`;
    this.state.self_link = result.selfLink;
    this.state.location = result.location;
    this.state.creation_time = result.creationTime;
    this.state.last_modified_time = result.lastModifiedTime;
    this.state.storage_billing_model = result.storageBillingModel;
    this.state.existing = false;
    cli.output(`Dataset created: ${this.state.dataset_reference}`);
    if (this.definition.tables) {
      this.createTables();
    }
  }
  /**
   * Create tables from definition
   */
  createTables() {
    if (!this.definition.tables) return;
    let tables;
    try {
      tables = JSON.parse(this.definition.tables);
    } catch (error) {
      cli.output(`Error parsing tables definition: ${error}`);
      return;
    }
    for (const table of tables) {
      cli.output(`Creating table: ${table.name}`);
      let fields;
      try {
        fields = JSON.parse(table.schema);
      } catch (error) {
        cli.output(`Error parsing schema for table ${table.name}: ${error}`);
        continue;
      }
      const body = {
        tableReference: {
          projectId: this.projectId,
          datasetId: this.definition.dataset,
          tableId: table.name
        },
        schema: {
          fields
        }
      };
      if (table.description) {
        body.description = table.description;
      }
      if (table.expirationMs) {
        body.expirationTime = (Date.now() + table.expirationMs).toString();
      }
      if (table.partitionField) {
        body.timePartitioning = {
          type: "DAY",
          field: table.partitionField
        };
      }
      if (table.clusteringFields && table.clusteringFields.length > 0) {
        body.clustering = {
          fields: table.clusteringFields
        };
      }
      this.post(
        `${this.apiUrl}/datasets/${this.definition.dataset}/tables`,
        body
      );
      cli.output(`Table ${table.name} created`);
    }
  }
  update() {
    const existing = this.getDataset();
    if (!existing) {
      cli.output("Dataset not found, creating...");
      this.create();
      return;
    }
    const body = {};
    if (this.definition.dataset_description) {
      body.description = this.definition.dataset_description;
    }
    if (this.definition.labels) {
      body.labels = this.definition.labels;
    }
    if (this.definition.default_table_expiration_ms) {
      body.defaultTableExpirationMs = this.definition.default_table_expiration_ms.toString();
    }
    if (Object.keys(body).length > 0) {
      this.patch(`${this.apiUrl}/datasets/${this.definition.dataset}`, body);
      cli.output(`Dataset ${this.definition.dataset} updated`);
    }
    this.state.dataset_id = existing.datasetReference.datasetId;
    this.state.dataset_reference = `${this.projectId}:${existing.datasetReference.datasetId}`;
    this.state.self_link = existing.selfLink;
    this.state.location = existing.location;
  }
  delete() {
    if (this.state.existing) {
      cli.output(
        `Dataset ${this.definition.dataset} was not created by this entity, skipping delete`
      );
      return;
    }
    const existing = this.getDataset();
    if (!existing) {
      cli.output(`Dataset ${this.definition.dataset} does not exist`);
      return;
    }
    this.deleteAllTables();
    cli.output(`Deleting BigQuery dataset: ${this.definition.dataset}`);
    this.httpDelete(`${this.apiUrl}/datasets/${this.definition.dataset}`);
    cli.output(`Dataset ${this.definition.dataset} deleted`);
  }
  checkReadiness() {
    const dataset = this.getDataset();
    if (!dataset) {
      cli.output("Dataset not found");
      return false;
    }
    this.state.dataset_id = dataset.datasetReference.datasetId;
    this.state.dataset_reference = `${this.projectId}:${dataset.datasetReference.datasetId}`;
    this.state.self_link = dataset.selfLink;
    this.state.location = dataset.location;
    this.state.creation_time = dataset.creationTime;
    this.state.last_modified_time = dataset.lastModifiedTime;
    this.state.storage_billing_model = dataset.storageBillingModel;
    cli.output(
      `Dataset ${this.definition.dataset} is ready in ${dataset.location}`
    );
    return true;
  }
  checkLiveness() {
    return this.getDataset() !== null;
  }
  getInfo(_args) {
    const dataset = this.getDataset();
    if (!dataset) {
      throw new Error("Dataset not found");
    }
    cli.output(JSON.stringify(dataset, null, 2));
  }
  listAllTables(_args) {
    const tables = this.listTables();
    cli.output(JSON.stringify(tables, null, 2));
  }
  createTable(args) {
    if (!args?.name || !args?.schema) {
      throw new Error("Required arguments: name, schema (JSON string)");
    }
    const fields = JSON.parse(args.schema);
    const body = {
      tableReference: {
        projectId: this.projectId,
        datasetId: this.definition.dataset,
        tableId: args.name
      },
      schema: {
        fields
      }
    };
    this.post(
      `${this.apiUrl}/datasets/${this.definition.dataset}/tables`,
      body
    );
    cli.output(`Table ${args.name} created`);
  }
  deleteTable(args) {
    if (!args?.name) {
      throw new Error("Required argument: name");
    }
    this.httpDelete(
      `${this.apiUrl}/datasets/${this.definition.dataset}/tables/${args.name}`
    );
    cli.output(`Table ${args.name} deleted`);
  }
  getBackupInfo(_args) {
    cli.output(`==================================================`);
    cli.output(`\u{1F4E6} Backup Information for BigQuery dataset`);
    cli.output(`Dataset: ${this.definition.dataset}`);
    cli.output(`Project: ${this.projectId}`);
    cli.output(`==================================================`);
    const dataset = this.getDataset();
    if (!dataset) {
      throw new Error(`Dataset ${this.definition.dataset} not found`);
    }
    cli.output(`
\u{1F527} Dataset Configuration:`);
    cli.output(`   Location: ${dataset.location || "US"}`);
    cli.output(`   Storage Billing Model: ${dataset.storageBillingModel || "LOGICAL"}`);
    const maxTimeTravelHours = dataset.maxTimeTravelHours || 168;
    const maxTimeTravelDays = Math.floor(maxTimeTravelHours / 24);
    cli.output(`   Time Travel Window: ${maxTimeTravelHours} hours (${maxTimeTravelDays} days)`);
    if (dataset.defaultTableExpirationMs) {
      const expDays = Math.floor(Number(dataset.defaultTableExpirationMs) / (1e3 * 60 * 60 * 24));
      cli.output(`   Default Table Expiration: ${expDays} days`);
    }
    cli.output(`
\u{1F4CB} BigQuery Backup Capabilities:`);
    cli.output(`   \u2705 Time Travel: Query data from up to ${maxTimeTravelDays} days ago`);
    cli.output(`   \u2705 Table Snapshots: Create point-in-time copies of tables`);
    cli.output(`   \u2705 Table Clones: Create lightweight copies for testing`);
    cli.output(`
\u{1F4CB} Available operations:`);
    cli.output(`   monk do namespace/dataset create-snapshot table="my_table" snapshot="my_table_backup"`);
    cli.output(`   monk do namespace/dataset list-snapshots`);
    cli.output(`   monk do namespace/dataset restore snapshot="my_table_backup" target="restored_table"`);
    cli.output(`
==================================================`);
  }
  createSnapshot(args) {
    cli.output(`==================================================`);
    cli.output(`Creating table snapshot in BigQuery`);
    cli.output(`Dataset: ${this.definition.dataset}`);
    cli.output(`Project: ${this.projectId}`);
    cli.output(`==================================================`);
    const sourceTable = args?.table;
    const snapshotName = args?.snapshot;
    if (!sourceTable) {
      throw new Error(
        `'table' is required.
Usage: monk do namespace/dataset create-snapshot table="source_table" snapshot="snapshot_name"`
      );
    }
    if (!snapshotName) {
      throw new Error(
        `'snapshot' is required.
Usage: monk do namespace/dataset create-snapshot table="source_table" snapshot="snapshot_name"`
      );
    }
    const targetDataset = args?.target_dataset || this.definition.dataset;
    const expirationDays = args?.expiration_days ? Number(args.expiration_days) : void 0;
    const snapshotTime = args?.snapshot_time;
    cli.output(`Source Table: ${sourceTable}`);
    cli.output(`Snapshot Name: ${snapshotName}`);
    cli.output(`Target Dataset: ${targetDataset}`);
    if (expirationDays) {
      cli.output(`Expiration: ${expirationDays} days`);
    }
    if (snapshotTime) {
      cli.output(`Snapshot Time: ${snapshotTime}`);
    }
    const jobBody = {
      configuration: {
        copy: {
          sourceTable: {
            projectId: this.projectId,
            datasetId: this.definition.dataset,
            tableId: sourceTable
          },
          destinationTable: {
            projectId: this.projectId,
            datasetId: targetDataset,
            tableId: snapshotName
          },
          operationType: "SNAPSHOT"
        }
      }
    };
    if (snapshotTime) {
      jobBody.configuration.copy.sourceTable.snapshotTime = snapshotTime;
    }
    if (expirationDays) {
      const expirationMs = Date.now() + expirationDays * 24 * 60 * 60 * 1e3;
      jobBody.configuration.copy.destinationExpirationTime = new Date(expirationMs).toISOString();
    }
    try {
      const jobResult = this.post(`${this.apiUrl}/jobs`, jobBody);
      cli.output(`
Job submitted: ${jobResult.jobReference?.jobId}`);
      cli.output(`Status: ${jobResult.status?.state || "PENDING"}`);
      const jobId = jobResult.jobReference?.jobId;
      if (jobId) {
        let attempts = 0;
        const maxAttempts = 30;
        let jobStatus = jobResult;
        while (jobStatus.status?.state !== "DONE" && attempts < maxAttempts) {
          const start = Date.now();
          while (Date.now() - start < 2e3) {
          }
          jobStatus = this.get(`${this.apiUrl}/jobs/${jobId}`);
          attempts++;
          if (jobStatus.status?.state === "DONE") {
            break;
          }
          cli.output(`   Waiting for job... (attempt ${attempts}/${maxAttempts})`);
        }
        if (jobStatus.status?.errorResult) {
          throw new Error(jobStatus.status.errorResult.message || "Job failed");
        }
        if (jobStatus.status?.state === "DONE") {
          cli.output(`
\u2705 Snapshot created successfully!`);
          cli.output(`Snapshot: ${snapshotName}`);
          cli.output(`Dataset: ${targetDataset}`);
          cli.output(`
\u{1F4CB} To restore from this snapshot:`);
          cli.output(`   monk do namespace/dataset restore snapshot="${snapshotName}" target="restored_table"`);
        } else {
          cli.output(`
\u23F3 Job still running. Check status with:`);
          cli.output(`   Use BigQuery console to monitor job ${jobId}`);
        }
      }
      cli.output(`==================================================`);
    } catch (error) {
      cli.output(`
\u274C Failed to create snapshot`);
      throw new Error(`Snapshot creation failed: ${error instanceof Error ? error.message : "Unknown error"}`);
    }
  }
  listSnapshots(args) {
    cli.output(`==================================================`);
    cli.output(`Listing table snapshots in BigQuery dataset`);
    cli.output(`Dataset: ${this.definition.dataset}`);
    cli.output(`Project: ${this.projectId}`);
    cli.output(`==================================================`);
    const limit = Number(args?.limit) || 10;
    try {
      const response = this.listTables();
      const allTables = response.tables || [];
      const snapshots = allTables.filter((t) => t.type === "SNAPSHOT");
      cli.output(`
Total snapshots found: ${snapshots.length}`);
      cli.output(`Showing: ${Math.min(snapshots.length, limit)} snapshot(s)
`);
      if (snapshots.length === 0) {
        cli.output(`No snapshots found in this dataset.`);
        cli.output(`
\u{1F4CB} To create a snapshot:`);
        cli.output(`   monk do namespace/dataset create-snapshot table="source_table" snapshot="snapshot_name"`);
      } else {
        const displaySnapshots = snapshots.slice(0, limit);
        for (let i = 0; i < displaySnapshots.length; i++) {
          const snapshot = displaySnapshots[i];
          const tableRef = snapshot.tableReference || {};
          cli.output(`\u{1F4F8} Snapshot #${i + 1}`);
          cli.output(`   Name: ${tableRef.tableId || "unknown"}`);
          cli.output(`   Type: ${snapshot.type}`);
          cli.output(`   Created: ${snapshot.creationTime ? new Date(Number(snapshot.creationTime)).toISOString() : "N/A"}`);
          if (snapshot.expirationTime) {
            cli.output(`   Expires: ${new Date(Number(snapshot.expirationTime)).toISOString()}`);
          }
          if (snapshot.snapshotDefinition?.baseTableReference) {
            const baseRef = snapshot.snapshotDefinition.baseTableReference;
            cli.output(`   Base Table: ${baseRef.datasetId}.${baseRef.tableId}`);
          }
          if (snapshot.snapshotDefinition?.snapshotTime) {
            cli.output(`   Snapshot Time: ${snapshot.snapshotDefinition.snapshotTime}`);
          }
          cli.output(``);
        }
        if (snapshots.length > limit) {
          cli.output(`... and ${snapshots.length - limit} more snapshot(s)`);
          cli.output(`Increase limit with: monk do namespace/dataset list-snapshots limit=${snapshots.length}`);
        }
      }
      cli.output(`==================================================`);
    } catch (error) {
      cli.output(`
\u274C Failed to list snapshots`);
      throw new Error(`List snapshots failed: ${error instanceof Error ? error.message : "Unknown error"}`);
    }
  }
  describeSnapshot(args) {
    cli.output(`==================================================`);
    cli.output(`\u{1F4F8} Snapshot Details`);
    cli.output(`==================================================`);
    const snapshotName = args?.snapshot;
    if (!snapshotName) {
      throw new Error(
        `'snapshot' is required.
Usage: monk do namespace/dataset describe-snapshot snapshot="snapshot_name"

To find snapshots, run: monk do namespace/dataset list-snapshots`
      );
    }
    try {
      const url = `${this.apiUrl}/datasets/${this.definition.dataset}/tables/${snapshotName}`;
      const snapshot = this.get(url);
      if (snapshot.type !== "SNAPSHOT") {
        cli.output(`
\u26A0\uFE0F  Warning: Table '${snapshotName}' is not a snapshot (type: ${snapshot.type})`);
      }
      cli.output(`
\u{1F4F8} Snapshot Information`);
      cli.output(`--------------------------------------------------`);
      cli.output(`Name: ${snapshot.tableReference?.tableId || snapshotName}`);
      cli.output(`Type: ${snapshot.type || "TABLE"}`);
      cli.output(`Dataset: ${snapshot.tableReference?.datasetId}`);
      cli.output(`Project: ${snapshot.tableReference?.projectId}`);
      cli.output(`Created: ${snapshot.creationTime ? new Date(Number(snapshot.creationTime)).toISOString() : "N/A"}`);
      cli.output(`Last Modified: ${snapshot.lastModifiedTime ? new Date(Number(snapshot.lastModifiedTime)).toISOString() : "N/A"}`);
      if (snapshot.expirationTime) {
        cli.output(`Expires: ${new Date(Number(snapshot.expirationTime)).toISOString()}`);
      }
      if (snapshot.numBytes) {
        const sizeGB = (Number(snapshot.numBytes) / (1024 * 1024 * 1024)).toFixed(4);
        cli.output(`Size: ${sizeGB} GB`);
      }
      if (snapshot.numRows) {
        cli.output(`Rows: ${snapshot.numRows}`);
      }
      if (snapshot.snapshotDefinition) {
        const snapDef = snapshot.snapshotDefinition;
        cli.output(`
\u{1F4CB} Snapshot Source:`);
        if (snapDef.baseTableReference) {
          const baseRef = snapDef.baseTableReference;
          cli.output(`   Base Table: ${baseRef.projectId}.${baseRef.datasetId}.${baseRef.tableId}`);
        }
        if (snapDef.snapshotTime) {
          cli.output(`   Snapshot Time: ${snapDef.snapshotTime}`);
        }
      }
      cli.output(`
\u{1F4CB} To restore from this snapshot:`);
      cli.output(`   monk do namespace/dataset restore snapshot="${snapshotName}" target="restored_table"`);
      cli.output(`
==================================================`);
    } catch (error) {
      cli.output(`
\u274C Failed to get snapshot details`);
      throw new Error(`Describe snapshot failed: ${error instanceof Error ? error.message : "Unknown error"}`);
    }
  }
  deleteSnapshot(args) {
    cli.output(`==================================================`);
    cli.output(`\u{1F5D1}\uFE0F DELETE SNAPSHOT - READ CAREFULLY!`);
    cli.output(`==================================================`);
    const snapshotName = args?.snapshot;
    if (!snapshotName) {
      throw new Error(
        `'snapshot' is required.
Usage: monk do namespace/dataset delete-snapshot snapshot="snapshot_name"

To find snapshots, run: monk do namespace/dataset list-snapshots`
      );
    }
    try {
      const url = `${this.apiUrl}/datasets/${this.definition.dataset}/tables/${snapshotName}`;
      const snapshot = this.get(url);
      cli.output(`
\u26A0\uFE0F  WARNING: This will permanently delete the snapshot!`);
      cli.output(`   Snapshot: ${snapshotName}`);
      cli.output(`   Type: ${snapshot.type || "TABLE"}`);
      cli.output(`   Created: ${snapshot.creationTime ? new Date(Number(snapshot.creationTime)).toISOString() : "N/A"}`);
      cli.output(`--------------------------------------------------`);
      this.httpDelete(url);
      cli.output(`
\u2705 Snapshot deleted successfully!`);
      cli.output(`==================================================`);
    } catch (error) {
      cli.output(`
\u274C Failed to delete snapshot`);
      throw new Error(`Delete snapshot failed: ${error instanceof Error ? error.message : "Unknown error"}`);
    }
  }
  restoreSnapshot(args) {
    cli.output(`==================================================`);
    cli.output(`\u{1F504} RESTORE TABLE FROM SNAPSHOT`);
    cli.output(`==================================================`);
    cli.output(`Dataset: ${this.definition.dataset}`);
    cli.output(`Project: ${this.projectId}`);
    const snapshotName = args?.snapshot;
    const targetTable = args?.target;
    if (!snapshotName) {
      throw new Error(
        `'snapshot' is required.
Usage: monk do namespace/dataset restore snapshot="snapshot_name" target="restored_table"

To find snapshots, run: monk do namespace/dataset list-snapshots`
      );
    }
    if (!targetTable) {
      throw new Error(
        "'target' is required.\nSpecify a name for the restored table."
      );
    }
    const targetDataset = args?.target_dataset || this.definition.dataset;
    cli.output(`
\u{1F4CB} Restore Configuration:`);
    cli.output(`   Source Snapshot: ${snapshotName}`);
    cli.output(`   Target Table: ${targetTable}`);
    cli.output(`   Target Dataset: ${targetDataset}`);
    cli.output(`--------------------------------------------------`);
    cli.output(`
\u26A0\uFE0F  NOTE: This will create a NEW writable table.`);
    cli.output(`   The original snapshot will NOT be affected.`);
    const jobBody = {
      configuration: {
        copy: {
          sourceTable: {
            projectId: this.projectId,
            datasetId: this.definition.dataset,
            tableId: snapshotName
          },
          destinationTable: {
            projectId: this.projectId,
            datasetId: targetDataset,
            tableId: targetTable
          },
          operationType: "RESTORE"
        }
      }
    };
    try {
      const jobResult = this.post(`${this.apiUrl}/jobs`, jobBody);
      cli.output(`
Job submitted: ${jobResult.jobReference?.jobId}`);
      cli.output(`Status: ${jobResult.status?.state || "PENDING"}`);
      const jobId = jobResult.jobReference?.jobId;
      if (jobId) {
        let attempts = 0;
        const maxAttempts = 30;
        let jobStatus = jobResult;
        while (jobStatus.status?.state !== "DONE" && attempts < maxAttempts) {
          const start = Date.now();
          while (Date.now() - start < 2e3) {
          }
          jobStatus = this.get(`${this.apiUrl}/jobs/${jobId}`);
          attempts++;
          if (jobStatus.status?.state === "DONE") {
            break;
          }
          cli.output(`   Waiting for job... (attempt ${attempts}/${maxAttempts})`);
        }
        if (jobStatus.status?.errorResult) {
          throw new Error(jobStatus.status.errorResult.message || "Job failed");
        }
        if (jobStatus.status?.state === "DONE") {
          cli.output(`
\u2705 Table restored successfully!`);
          cli.output(`Restored Table: ${targetTable}`);
          cli.output(`Dataset: ${targetDataset}`);
          cli.output(`
\u{1F4CB} The restored table is now available for queries:`);
          cli.output(`   SELECT * FROM \`${this.projectId}.${targetDataset}.${targetTable}\``);
        } else {
          cli.output(`
\u23F3 Job still running. Check status with BigQuery console.`);
        }
      }
      cli.output(`
==================================================`);
    } catch (error) {
      cli.output(`
\u274C Failed to restore from snapshot`);
      throw new Error(`Restore failed: ${error instanceof Error ? error.message : "Unknown error"}`);
    }
  }
  timeTravelInfo(args) {
    cli.output(`==================================================`);
    cli.output(`\u23F0 Time Travel Information`);
    cli.output(`==================================================`);
    const tableName = args?.table;
    if (!tableName) {
      throw new Error(
        `'table' is required.
Usage: monk do namespace/dataset time-travel-info table="my_table"`
      );
    }
    const dataset = this.getDataset();
    if (!dataset) {
      throw new Error(`Dataset ${this.definition.dataset} not found`);
    }
    const maxTimeTravelHours = dataset.maxTimeTravelHours || 168;
    const maxTimeTravelDays = Math.floor(maxTimeTravelHours / 24);
    cli.output(`
\u{1F4CB} Time Travel Configuration:`);
    cli.output(`   Dataset: ${this.definition.dataset}`);
    cli.output(`   Table: ${tableName}`);
    cli.output(`   Storage Billing: ${dataset.storageBillingModel || "LOGICAL"}`);
    cli.output(`   Max Time Travel: ${maxTimeTravelHours} hours (${maxTimeTravelDays} days)`);
    cli.output(`
\u{1F4CB} Time Travel Query Examples:`);
    cli.output(`
   Query data from 1 hour ago:`);
    cli.output(`   SELECT * FROM \`${this.projectId}.${this.definition.dataset}.${tableName}\``);
    cli.output(`   FOR SYSTEM_TIME AS OF TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 1 HOUR)`);
    cli.output(`
   Query data from 1 day ago:`);
    cli.output(`   SELECT * FROM \`${this.projectId}.${this.definition.dataset}.${tableName}\``);
    cli.output(`   FOR SYSTEM_TIME AS OF TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 1 DAY)`);
    cli.output(`
   Query data at a specific time:`);
    cli.output(`   SELECT * FROM \`${this.projectId}.${this.definition.dataset}.${tableName}\``);
    cli.output(`   FOR SYSTEM_TIME AS OF TIMESTAMP("2024-01-15 10:00:00 UTC")`);
    cli.output(`
\u{1F4CB} Create a snapshot from a point in time:`);
    cli.output(`   monk do namespace/dataset create-snapshot table="${tableName}" snapshot="${tableName}_backup" snapshot_time="2024-01-15T10:00:00Z"`);
    cli.output(`
==================================================`);
  }
};
_init = __decoratorStart(_a);
__decorateElement(_init, 1, "getInfo", _getInfo_dec, _BigQuery);
__decorateElement(_init, 1, "listAllTables", _listAllTables_dec, _BigQuery);
__decorateElement(_init, 1, "createTable", _createTable_dec, _BigQuery);
__decorateElement(_init, 1, "deleteTable", _deleteTable_dec, _BigQuery);
__decorateElement(_init, 1, "getBackupInfo", _getBackupInfo_dec, _BigQuery);
__decorateElement(_init, 1, "createSnapshot", _createSnapshot_dec, _BigQuery);
__decorateElement(_init, 1, "listSnapshots", _listSnapshots_dec, _BigQuery);
__decorateElement(_init, 1, "describeSnapshot", _describeSnapshot_dec, _BigQuery);
__decorateElement(_init, 1, "deleteSnapshot", _deleteSnapshot_dec, _BigQuery);
__decorateElement(_init, 1, "restoreSnapshot", _restoreSnapshot_dec, _BigQuery);
__decorateElement(_init, 1, "timeTravelInfo", _timeTravelInfo_dec, _BigQuery);
__decoratorMetadata(_init, _BigQuery);
__name(_BigQuery, "BigQuery");
__publicField(_BigQuery, "readiness", { period: 5, initialDelay: 2, attempts: 10 });
var BigQuery = _BigQuery;



function main(def, state, ctx) {
  const entity = new BigQuery(def, state, ctx);
  return entity.main(ctx);
}
